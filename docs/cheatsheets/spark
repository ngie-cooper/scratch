## pyspark

### Introduction

The pyspark shell provided by the apache-spark homebrew formula seems to run
python 2.7, so some additional features will need to be pulled in for py3
goodness:

```
>>> from __future__ import print_function
```

Much of the inspiration/poking around comes from
*Learning Spark, Lightning-Fast Big Data Analysis* (O'Reilly).

Some of the tests were run from an apache/spark checkout and differed from the
book, due to its age and the evolution of the project:
```
$ git log -n 1 --format=oneline master
61561c1c2d4e47191fdfe9bf3539a3db29e89fa9 (HEAD -> master, origin/master, origin/HEAD) [SPARK-27252][SQL][FOLLOWUP] Calculate min and max days independently from time zone in ComputeCurrentTimeSuite
```

### Manipulating RDDs

```
>>> lines = sc.textFile("README.md")
>>> lines_with_python = lines.filter(lambda v: "Python" in v)
>>> lines_with_python.foreach(lambda v: print(v))
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
## Interactive Python Shell
Alternatively, if you prefer Python, you can use the Python shell:
>>> lines_with_python.take(3)
[u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', u'## Interactive Python Shell', u'Alternatively, if you prefer Python, you can use the Python shell:']
>>> lines_with_python.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'
>>> lines.first()
u'# Apache Spark'
>>> lines.count()
109
>>> a_range_rdd = sc.parallelize(range(20))
>>> a_range_rdd.map(lambda v: v * v).collect()
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361]
>>> a_range_rdd.map(lambda v: v * v).foreach(print)
100
121
256
289
324
361
144
169
196
225
16
25
0
1
4
9
36
49
64
81
```

In the above lines, per the book, `.count()`, `.first()`, and `.take(3)` are
actions, whereas `.filter()` and `.map()` are transforms (the former return
non-RDD types, whereas the latter return RDD types).

Interestingly enough, `.collect()` seems to serialize the values in a FIFO
manner, whereas `.foreach()` seems to handle them asynchronously. It might be
because `.collect()` is an action, whereas `.foreach()` is a transform.

```
>>> rdd = sc.textFile("CONTRIBUTING.md")
>>> result = words.map(lambda x: (x, 1 )).reduceByKey(lambda x, y: x + y)
>>> result.countByKey()
defaultdict(<type 'int'>, {u'': 1, u'email,': 1, u'proposed': 1, u'being': 1, u'motivated?': 1, u'copyrighted': 1, u'before': 1, u'[Contributing': 1, u'enough': 1, u'to': 1, u'under': 1, u'guide](http://spark.apache.org/contributing.html).': 1, u'warrant': 1, u'do': 1, u'means': 1, u'Whether': 1, u'material': 1, u'not': 1, u'you': 1, u'In': 1, u'steps': 1, u'##': 1, u'authority': 1, u'are': 1, u'license.': 1, u'particular,': 1, u'request,': 1, u'for': 1, u'review': 1, u'legal': 1, u'state': 1, u'new': 1, u'?': 1, u'creating': 1, u'opening': 1, u'explicitly,': 1, u'ask': 1, u'by': 1, u'change': 1, u'request*,': 1, u'license': 1, u'agree': 1, u'stand': 1, u'contribution': 1, u"project's": 1, u'or': 1, u'via': 1, u'explained': 1, u'When': 1, u'community': 1, u'Contributing': 1, u'alone': 1, u'open': 1, u'your': 1, u'source': 1, u'that': 1, u'lists': 1, u'so.': 1, u'consider:': 1, u'related': 1, u'requests?': 1, u'pull': 1, u'*Before': 1, u'affirm': 1, u'this': 1, u'work': 1, u'project': 1, u'can': 1, u'submitting': 1, u'Is': 1, u'and': 1, u'is': 1, u'contribute': 1, u'as': 1, u'have': 1, u'ready': 1, u'clearly': 1, u'[third': 1, u'-': 1, u'feature': 1, u'any': 1, u'other': 1, u'party': 1, u'Spark': 1, u'PR.': 1, u'searched': 1, u'important': 1, u'JIRAs': 1, u'a': 1, u'reviewing?': 1, u'project](http://spark.apache.org/third-party-projects.html)': 1, u'required': 1, u'It': 1, u'spend': 1, u'existing,': 1, u'Have': 1, u'time': 1, u'the': 1, u'code,': 1, u'original': 1})
>>> rdd.flatMap(lambda x: x.split()).countByValue()
defaultdict(<type 'int'>, {u'email,': 1, u'proposed': 1, u'being': 1, u'motivated?': 1, u'copyrighted': 1, u'before': 1, u'[Contributing': 1, u'state': 1, u'source': 2, u'under': 2, u'guide](http://spark.apache.org/contributing.html).': 1, u'warrant': 1, u'do': 1, u'means': 1, u'Whether': 1, u'material': 2, u'not': 1, u'you': 7, u'steps': 1, u'##': 1, u'authority': 1, u'are': 1, u'license.': 1, u'project': 1, u'request,': 1, u'for': 1, u'review': 1, u'legal': 1, u'enough': 1, u'new': 1, u'?': 1, u'creating': 1, u'opening': 1, u'explicitly,': 1, u'ask': 1, u'by': 1, u'change': 2, u'request*,': 1, u'license': 3, u'agree': 1, u'stand': 1, u'JIRAs': 1, u"project's": 2, u'or': 2, u'via': 1, u'explained': 1, u'When': 1, u'community': 1, u'Contributing': 1, u'alone': 1, u'open': 2, u'your': 1, u'to': 7, u'time': 1, u'that': 5, u'lists': 1, u'so.': 1, u'consider:': 1, u'related': 1, u'requests?': 1, u'pull': 3, u'*Before': 1, u'affirm': 1, u'this': 2, u'work': 2, u'particular,': 1, u'can': 1, u'submitting': 1, u'Is': 3, u'and': 5, u'is': 1, u'contribute': 1, u'as': 1, u'have': 1, u'ready': 1, u'clearly': 1, u'existing,': 1, u'-': 4, u'feature': 1, u'any': 1, u'other': 1, u'party': 1, u'Spark': 2, u'PR.': 1, u'In': 1, u'important': 1, u'contribution': 1, u'a': 4, u'reviewing?': 1, u'project](http://spark.apache.org/third-party-projects.html)': 1, u'required': 1, u'It': 1, u'original': 1, u'[third': 1, u'Have': 1, u'searched': 1, u'the': 11, u'code,': 1, u'spend': 1})
```
